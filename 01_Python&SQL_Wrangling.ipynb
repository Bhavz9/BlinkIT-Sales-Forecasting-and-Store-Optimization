{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "84778b31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running SQL data standardization...\n",
      "Running SQL imputation for Item Weight...\n",
      "\n",
      "Top Outlet Sizes by Type (for manual imputation logic):\n",
      "         Outlet Type Outlet Size  count\n",
      "0      Grocery Store       Small    585\n",
      "1      Grocery Store      Medium    265\n",
      "2      Grocery Store        High    233\n",
      "3  Supermarket Type1       Small   2554\n",
      "4  Supermarket Type1        High   1520\n",
      "5  Supermarket Type1      Medium   1503\n",
      "6  Supermarket Type2      Medium    928\n",
      "7  Supermarket Type3      Medium    935\n",
      "\n",
      "--- SQL Wrangling Complete ---\n",
      "Cleaned DataFrame Head:\n",
      "  Item Fat Content Item Identifier              Item Type  \\\n",
      "0          Regular           FDX32  Fruits and Vegetables   \n",
      "1          Low Fat           NCB42     Health and Hygiene   \n",
      "2          Regular           FDR28           Frozen Foods   \n",
      "3          Regular           FDL50                 Canned   \n",
      "4          Low Fat           DRI25            Soft Drinks   \n",
      "\n",
      "   Outlet Establishment Year Outlet Identifier Outlet Location Type  \\\n",
      "0                       2012            OUT049               Tier 1   \n",
      "1                       2022            OUT018               Tier 3   \n",
      "2                       2016            OUT046               Tier 1   \n",
      "3                       2014            OUT013               Tier 3   \n",
      "4                       2015            OUT045               Tier 2   \n",
      "\n",
      "  Outlet Size        Outlet Type  Item Visibility  Item Weight     Sales  \\\n",
      "0      Medium  Supermarket Type1         0.100014        15.10  145.4786   \n",
      "1      Medium  Supermarket Type2         0.008596        11.80  115.3492   \n",
      "2       Small  Supermarket Type1         0.025896        13.85  165.0210   \n",
      "3        High  Supermarket Type1         0.042278        12.15  126.5046   \n",
      "4       Small  Supermarket Type1         0.033970        19.60   55.1614   \n",
      "\n",
      "   Rating  \n",
      "0     5.0  \n",
      "1     5.0  \n",
      "2     5.0  \n",
      "3     5.0  \n",
      "4     5.0  \n",
      "\n",
      "Unique Item Fat Content after standardization: ['Regular' 'Low Fat']\n",
      "Missing Item Weight after imputation: 4\n",
      "Missing Outlet Size: 0\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import sqlite3\n",
    "import numpy as np\n",
    "\n",
    "file_path= r\"C:\\Users\\NNG Mathur\\Desktop\\Blinkit\\BlinkIT_Grocery_Data.xlsx\"\n",
    "sheet_name=\"BlinkIT Grocery Data\"\n",
    "\n",
    "try:\n",
    "    df_raw = pd.read_excel(file_path, sheet_name=sheet_name)\n",
    "except FileNotFoundError:\n",
    "    # Handle the case if the file path or sheet name is wrong.\n",
    "    print(f\"Error: File not found or sheet name '{sheet_name}' is incorrect. Please check the path.\")\n",
    "    exit()\n",
    "\n",
    "conn=sqlite3.connect(':memory:')\n",
    "table_name='raw_sales_data'\n",
    "\n",
    "df_raw.to_sql(table_name,conn,index=False,if_exists='replace')\n",
    "\n",
    "#1. SQL Cleaning and Standardization (Demonstrates SQL DML skills) \n",
    "print(\"Running SQL data standardization...\")\n",
    "update_fat_content_query_lf = f\"\"\"\n",
    "UPDATE {table_name}\n",
    "set \"Item Fat Content\"='Low Fat'\n",
    "WHERE \"Item Fat Content\" IN ('low fat', 'LF');\n",
    "\"\"\"\n",
    "conn.execute(update_fat_content_query_lf)\n",
    "\n",
    "update_fat_content_query_reg = f\"\"\"\n",
    "UPDATE {table_name}\n",
    "SET \"Item Fat Content\" = 'Regular'\n",
    "WHERE \"Item Fat Content\" = 'reg';\n",
    "\"\"\"\n",
    "conn.execute(update_fat_content_query_reg)\n",
    "\n",
    "print(\"Running SQL imputation for Item Weight...\")\n",
    "impute_weight_query = f\"\"\"\n",
    "update {table_name} as t1\n",
    "set \"Item Weight\"=(\n",
    "select avg(t2.\"Item Weight\")\n",
    "from {table_name} as t2\n",
    "where t2.\"Item Identifier\"=t1.\"Item Identifier\"\n",
    ")\n",
    "where t1.\"Item Weight\" is null;\n",
    "\"\"\"\n",
    "\n",
    "conn.execute(impute_weight_query)\n",
    "\n",
    "mode_size_query=f\"\"\"\n",
    "select \"Outlet Type\", \"Outlet Size\", count(*) as count \n",
    "from {table_name}\n",
    "where \"Outlet Size\" is not null\n",
    "group by \"Outlet Type\", \"Outlet Size\"\n",
    "order by \"Outlet Type\", count desc;\n",
    "\"\"\"\n",
    "print(\"\\nTop Outlet Sizes by Type (for manual imputation logic):\")\n",
    "print(pd.read_sql(mode_size_query, conn))\n",
    "\n",
    "#2. Extract the cleaned data into a new Pandas DataFrame   \n",
    "select_all_query=f\"select * from {table_name};\"\n",
    "df_cleaned=pd.read_sql(select_all_query,conn)\n",
    "\n",
    "conn.close()\n",
    "\n",
    "print(\"\\n--- SQL Wrangling Complete ---\")\n",
    "print(\"Cleaned DataFrame Head:\")\n",
    "print(df_cleaned.head())\n",
    "print(\"\\nUnique Item Fat Content after standardization:\", df_cleaned['Item Fat Content'].unique())\n",
    "print(\"Missing Item Weight after imputation:\", df_cleaned['Item Weight'].isnull().sum())\n",
    "# Note: You may still have missing Outlet_Size if an Outlet_Type has no recorded size.\n",
    "print(\"Missing Outlet Size:\", df_cleaned['Outlet Size'].isnull().sum())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Item Weight Imputation: Missing values are now 0.\n",
      "Outlet Size Imputation Complete: Missing values are now 0.\n",
      "Outlet Sizes after imputation: ['Medium' 'Small' 'High']\n",
      "\n",
      "--- Final Data Prep Summary ---\n",
      "  Item Fat Content Item Identifier              Item Type Outlet Identifier  \\\n",
      "0          Regular           FDX32  Fruits and Vegetables            OUT049   \n",
      "1          Low Fat           NCB42     Health and Hygiene            OUT018   \n",
      "2          Regular           FDR28           Frozen Foods            OUT046   \n",
      "3          Regular           FDL50                 Canned            OUT013   \n",
      "4          Low Fat           DRI25            Soft Drinks            OUT045   \n",
      "\n",
      "  Outlet Location Type Outlet Size        Outlet Type  Item Visibility  \\\n",
      "0               Tier 1      Medium  Supermarket Type1         0.100014   \n",
      "1               Tier 3      Medium  Supermarket Type2         0.008596   \n",
      "2               Tier 1       Small  Supermarket Type1         0.025896   \n",
      "3               Tier 3        High  Supermarket Type1         0.042278   \n",
      "4               Tier 2       Small  Supermarket Type1         0.033970   \n",
      "\n",
      "   Item Weight  Rating  Outlet_Age  Item_Visibility_Log  Sales_Log  \n",
      "0        15.10     5.0          12            -2.302450   4.986879  \n",
      "1        11.80     5.0           2            -4.756452   4.756596  \n",
      "2        13.85     5.0           8            -3.653648   5.112114  \n",
      "3        12.15     5.0          10            -3.163492   4.848152  \n",
      "4        19.60     5.0           9            -3.382272   4.028230  \n",
      "\n",
      "New features created: ['Outlet_Age', 'Item_Visibility_Log', 'Sales_Log']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\NNG Mathur\\AppData\\Local\\Temp\\ipykernel_16900\\3768736371.py:7: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df_cleaned['Item Weight'].fillna(median_item_weight, inplace=True)\n",
      "C:\\Users\\NNG Mathur\\AppData\\Local\\Temp\\ipykernel_16900\\3768736371.py:19: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df_cleaned['Outlet Size'].fillna('Small', inplace=True)\n"
     ]
    }
   ],
   "source": [
    "# Assuming 'df_cleaned' is the DataFrame resulting from your SQL code\n",
    "\n",
    "# 1. FINAL IMPUTATION FOR 'Item Weight'\n",
    "# The remaining 4 missing values belong to Item_Identifiers \n",
    "# that only had NULL weights. We'll impute them with the overall median.\n",
    "median_item_weight = df_cleaned['Item Weight'].median()\n",
    "df_cleaned['Item Weight'].fillna(median_item_weight, inplace=True)\n",
    "print(f\"Final Item Weight Imputation: Missing values are now {df_cleaned['Item Weight'].isnull().sum()}.\")\n",
    "\n",
    "# 2. IMPUTATION FOR 'Outlet Size'\n",
    "# We'll use the modal relationship observed in your SQL output to impute missing sizes.\n",
    "# Based on your SQL output:\n",
    "# - Outlet Type 'Grocery Store' modal size is 'Small'.\n",
    "# - Supermarket Type 1, 2, 3 all had 'Medium' as a high-frequency size, \n",
    "#   but since Outlet Type OUT019 (a Grocery Store) had missing size and 'Grocery Store' is mostly 'Small',\n",
    "#   we'll impute 'Outlet Size' = 'Small' for missing values.\n",
    "\n",
    "# Note: In the original data, Outlet_Identifier 'OUT010' and 'OUT019' (both Grocery Store) have missing 'Outlet Size' (or are null/small, depending on the Excel mapping)\n",
    "df_cleaned['Outlet Size'].fillna('Small', inplace=True)\n",
    "print(f\"Outlet Size Imputation Complete: Missing values are now {df_cleaned['Outlet Size'].isnull().sum()}.\")\n",
    "print(f\"Outlet Sizes after imputation: {df_cleaned['Outlet Size'].unique()}\")\n",
    "\n",
    "\n",
    "# 3. FEATURE ENGINEERING: Calculate Outlet Age\n",
    "current_year = 2024 # Assuming current year is 2024\n",
    "df_cleaned['Outlet_Age'] = current_year - df_cleaned['Outlet Establishment Year']\n",
    "df_cleaned.drop('Outlet Establishment Year', axis=1, inplace=True)\n",
    "\n",
    "\n",
    "# 4. FEATURE ENGINEERING: Handle zero 'Item Visibility' and Log Transform 'Sales'\n",
    "\n",
    "# Replace zero visibility with a tiny value to avoid log(0) issues, and then log-transform it\n",
    "df_cleaned['Item Visibility'] = np.where(\n",
    "    df_cleaned['Item Visibility'] == 0,\n",
    "    1e-9,  # Use 1e-9 (a very small number)\n",
    "    df_cleaned['Item Visibility']\n",
    ")\n",
    "df_cleaned['Item_Visibility_Log'] = np.log(df_cleaned['Item Visibility'])\n",
    "\n",
    "# Log transform the target variable ('Sales') to normalize the distribution\n",
    "df_cleaned['Sales_Log'] = np.log1p(df_cleaned['Sales']) \n",
    "df_cleaned.drop('Sales', axis=1, inplace=True) # Drop the original Sales column\n",
    "\n",
    "print(\"\\n--- Final Data Prep Summary ---\")\n",
    "print(df_cleaned.head())\n",
    "print(f\"\\nNew features created: ['Outlet_Age', 'Item_Visibility_Log', 'Sales_Log']\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✅ Cleaned data successfully exported as: blinkit_cleaned_data_for_ml.csv\n"
     ]
    }
   ],
   "source": [
    "# --- ADD THIS TO THE END OF YOUR PHASE 1 NOTEBOOK ---\n",
    "\n",
    "# 1. Define the name of the file to save the cleaned data\n",
    "output_file_name = 'blinkit_cleaned_data_for_ml.csv'\n",
    "\n",
    "# 2. Save the final DataFrame to a CSV file.\n",
    "# We set index=False because the index isn't needed as a column.\n",
    "df_cleaned.to_csv(output_file_name, index=False)\n",
    "\n",
    "print(f\"\\n✅ Cleaned data successfully exported as: {output_file_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f943d86a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
